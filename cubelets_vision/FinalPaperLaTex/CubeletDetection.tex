%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[conference]{IEEEtran}
% Add the compsoc option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
\usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
%\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/





% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.



%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later 
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/




% *** FLOAT PACKAGES ***
%
\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Cubelet Detection and Localization by Template Matching}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Charles Chen}
\IEEEauthorblockA{Electrical and Computer Engineering\\
University of Colorado at Boulder\\
Boulder, CO 80302}
\and
\IEEEauthorblockN{Charles Dietrich}
\IEEEauthorblockA{Computer Science\\
University of Colorado at Boulder\\
Boulder, CO 80302}
\and
\IEEEauthorblockN{Chris Miller}
\IEEEauthorblockA{Computer Science\\
University of Colorado at Boulder\\
Boulder, CO 80302}
}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle


\begin{abstract}
%\boldmath
We describe a method for detecting and localizing Modular Robotics Cubelets in a scene, for use as a subsystem in a system for autononmous assembly of Cubelets constructions using a robotic arm and grasper. We use a camera mounted over the work surface. Our method returns two-dimensional position and angle of rotation as well as the type of Cubelet. We obtain position and rotation using template matching on images preprocessed with a novel line detection algorithm. We obtain the type of Cubelet by using color information. We also describe a method to obtain depth information from a camera with an RGB-D sensor such as the Xbox Kinect.
\end{abstract}
% IEEEtran.cls defaults to using nonbold math in the Abstract.
% This preserves the distinction between vectors and scalars. However,
% if the conference you are submitting to favors bold math in the abstract,
% then you can use LaTeX's standard command \boldmath at the very start
% of the abstract to achieve this. Many IEEE journals/conferences frown on
% math in the abstract anyway.

% no keywords



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% no \IEEEPARstart
The purpose of this research is to produce a vision subsystem to the system necessary to accomplish the overall goal of 'robots building robots'.  The 'robots building robots' project involves the assembly of robots made up of Modular Robotics Cubelets 
\cite{Cubelets}. Cubelets are a robotic toy consisting of a set of small cubes. Each cube, or Cubelet, is approximately 50mm on a side. Cubelets attach to one another by way of keyed magnetic faces. Each Cubelet contains a small computer. A Cubelet may be an actuator, a sensor, a thinking block or a battery block. Actuators have an actuation face and respond to incoming signals. Sensors have a sensor face and sense the environment and send out a signal. Thinking blocks change the signal received from neighbors in a predetermined way, e.g. a Inverse block inverts the signal received. The battery block provides power.\\

For the purposes of the overall goal of 'robots building robots', we are concerned with autonmously assembling the Cubelets into a predetermined structure that forms a robot. We assume that the Cubelets start scattered over the work surface, a flat, featureless surface. We intend to build the construction using the Clam arm \cite{Clam}, a small robotic arm intended for research, with a suitable grasper. We also intend to use ROS (Robot Operating System) \cite{ROS} to bring together the entire system.\\

We assume that a vision subsystem is a necessary subsystem of the overall system. The vision subsystem will be used to direct the arm and grasper. The vision subsystem reifies visual and aligned depth camera data into an ontology that represents the arrangement of Cubelets on the work surface. This ontology can then be consumed by other parts of the system.\\

Our engineering solution provides a partial technology for providing a useful ontology. The ontology should contain position and rotation for each Cubelet as well as the type of Cubelet. Our technology aims to provide two dimensional position and rotation information and partial information on the type of Cubelet at a reasonable frame rate.\\

We hypothesize that template matching on images preprocessed with a novel line detection algorithm can provide position and orientation data. We hypothesize that color matching on matched areas can provide partial information on the type of Cubelet.

\subsection{Background}
The problem of providing the desired ontology is largely one of object detection, a well-known problem in computer vision. Object detection methods include feature detection methods such as SIFT \cite{SIFT} and SURF \cite{SURF} as well as template matching, segmentation and classification of segments, and flexible template matching. Feature based-methods are generally faster than template matching. We present evidence that SURF does not work well with Cubelets due to their particular appearance.

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation Results}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command, the
% \label for the overall figure must come after \caption.
% \hfil must be used as a separator to get equal spacing.
% The subfigure.sty package works much the same way, except \subfigure is
% used instead of \subfloat.
%

\begin{figure*}[!t]
\centerline{
\subfloat[Canny Edge Detection]
{\includegraphics[width=2.5in]{Canny_50_100_c.jpg}}
\label{fig_first_case}
\hfil
\subfloat[Sobel Edge Detection]{\includegraphics[width=2.5in]{Sobel_1_1_c}%
\label{fig_second_case}}}
\caption{Edge Detectors}
\label{fig_sim}
\end{figure*}

\begin{figure*}[!t]
\centerline{
\subfloat[Initial Edge Enhancement Filter]
{\includegraphics[width=2.5in]{vh1_c.jpg}}
\label{fig_first_case}
\hfil
\subfloat[Low Noise Edge Enhancement Filter]{\includegraphics[width=2.5in]{vh2_c.jpg}%
\label{fig_second_case}}}
\caption{Our Edge Enhancement Filters}
\label{fig_sim}
\end{figure*}

%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.


% An example of a floating table. Note that, for IEEE style tables, the 
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that IEEE does not put floats in the very first column - or typically
% anywhere on the first page for that matter. Also, in-text middle ("here")
% positioning is not used. Most IEEE journals/conferences use top floats
% exclusively. Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the \fnbelowfloat
% command of the stfloats package.

\section{Materials and Methods}
Our experimental setup consisted of a Xbox Kinect camera \cite{Kinect} mounted approximately 65cm above the work surface. This distance was far enough for the depth sensor to work even if the blocks are stacked, but close enough to make out features on the faces of the Cubelets. The work surface was covered in white paper. The lighting was not controlled but was normal office lighting.\\

We generated three sets of 10-14 images each. Within each set, the images were taken with the same setup and lighting conditions. Each image takes a picture of an arrangement of Cubelets. The Cubelet arrangements were randomly determined by us.\\

Our software is written in Python and uses ROS with OpenCV \cite{OpenCV} and OpenNI \cite{OpenNI}.

\subsection{General Methods}
One image from each set was used as ground truth. For this image, we hand-labeled the location of one Cubelet of each color with (x,y) information. For each set of images, we determined a template radius, corresponding roughly to the circular portion of the cube face.  

\subsection{SURF Feature Detection}
The first method we attempted was trying to match SURF features between a template image consisting of a single cube and test images of a scene containing one or more Cubelets. We were able to obtain many features for each Cubelet. The first issue we found was the 4-way rotational symmetry of the Cubelets' faces caused erroneous feature matches. In an effort to fix this problem, we selected only features on one quarter of the face. However, we found that even then the features were too easily confused with each other, i.e. the features were not stable.\\

We also attempted to use SURF features to detect the Cubelets through their rotational symmetry.  Because they are rotationally symmetrical at 90 degrees, we assumed this would make the features appear more stable. This method has been effective for other scenarios \cite{Loy_Eklundh}, but the lack of stable features in Cubelets was still too much of a problem for it to work here. Additionally, this method finds symmetric matches other than the Cubelet faces.\\ 

{\includegraphics[width=2.5in]{surf.png}} \\


\begin{figure*}[!t]
\centerline{
\subfloat[Original Image]
{\includegraphics[width=2.5in]{Rotation_Ex_1.png}}
\label{fig_first_case}
\hfil
\subfloat[After Edge Enhancement Filter]{\includegraphics[width=2.5in]{Rotation_Ex_2.png}%
\label{fig_second_case}}}
\caption{Rotation Estimation - Initial Processing}
\label{fig_sim}
\end{figure*}

\begin{figure*}[!t]
\centerline{
\subfloat[After Successive Blurs, Dilations, and Erosions]
{\includegraphics[width=2.5in]{Rotation_Ex_3.png}}
\label{fig_first_case}
\hfil
\subfloat[After Canny Edge Detection]{\includegraphics[width=2.5in]{Rotation_Ex_4.png}%
\label{fig_second_case}}}
\caption{Rotation Estimation - Cube Outlining}
\label{fig_sim}
\end{figure*}

\begin{figure*}[!h]
\centerline{
\subfloat[Result of Hough Line Detection]
{\includegraphics[width=2.5in]{Rotation_Ex_5.png}}
\label{fig_first_case}
\hfil
\subfloat[Final Estimation]{\includegraphics[width=2.5in]{Rotation_Ex_6.png}%
\label{fig_second_case}}}
\caption{Rotation Estimation - Line Detection and Estimation}
\label{fig_sim}
\end{figure*} 

\subsection{Template Matching}

We then attempted to match faces to templates, first based on grayscale images and then based on RGB images. These methods were promising.\\

In order to better match the Cubelet faces, specifically the clear faces, we applied a novel form of edge detection/enhancement to the images. Without doing this, the features of the clear Cubelets were not very distinct.  We found the Canny edge detector \cite{Canny} to be too imprecise; a slight adjustment to the upper and lower edge merge thresholds would change the shape of fine detail edges. The Sobel edge detector \cite{Sobel} suffered from a different problem in that the edges that it did detect were very weakly enhanced.  In order to improve the edge enhancement, we  wrote our own novel filter which is very similar to how Sobel operates.  Shown are examples of Sobel and Canny run on our test images, as well as the two versions of our edge enhancement filter.  \\

The first variant of our filter behaves as follows: for each pixel, the value of at pixel becomes the product of the difference between the pixels above and below and the difference between the pixel to the left and the right.
\\[6pt]
\begin{math} 
	f(x,y) = |i(x+1,y) - i(x-1,y)|  \times |i(x,y+1) - i(x,y-1)| 
\end{math} 
\\[6pt]
This filter enhances edge boundaries very strongly, but still suffers from a bit of noise. The second filter reduces the noise seen, but as a side effect it reduces some of the edge enhancement strength. 
\\[6pt]
\begin{math} 
	f(x,y) = \frac{1}{c} |i(x+1,y) - i(x,y)|  \times |i(x-1,y) - i(x,y)| \times |i(x,y+1) - i(x,y)| \times |i(x,y-1) - i(x,y)| 
\end{math} 
\\[6pt]

Before performing template matching, we apply these filters to the template image and the target image. This template matching worked well, but did not accurately find matches of the correct color.

In order to handle the possibility of missing a match due to a rotation, we took each template image, rotated it by 15 degrees, and then template matched that rotated version to the target image.

To accomplish the actual template matching, we used the template matching function built into OpenCV, cv.MatchTemplate. The particular match value calculation equation we used, CV\_TM\_SQDIFF\_NORMED,  is as follows: 
\\[6pt]
\begin{math} 
	R(x,y) = \frac{\sum_{x',y'}(T(x',y')-I(x+x',y+y'))^{2}}{\sqrt{\sum_{x',y'}T(x',y')^{2}\cdot \sum_{x',y'}I(x+x',y+y')^{2} }}
\end{math} 
\\[6pt]
Here, $T(x,y)$ refers to a pixel value within the template image at $(x,y)$, and $I(x,y)$ refers to a pixel value in the target image. The output of this function is a grayscale image with pixel values equal to difference metric $R(x,y)$ between the local window and the template.  To find the matches in the image, we simply look for the minima within the resulting image.

Once a match was found in the match image, we excluded a circular area around the match from further matches.

\subsection{Rotation Estimation}
In order to determine the rotation of each Cubelet, we used the relatively distinct edges of the Cubelet.  We started by applying our edge enhancement filter to the test image.  We then apply blur, dilate, and erode operations to the resulting image in order to merge the edges of each Cubelet into a single connect component.  This supresses the internal features of the Cubelet, such as the metal connectors. We then applied Canny edge detection to the image, resulting in only the outlines of the cubes.  \\

After Canny edge detection has been applied, we used the Hough line detector \cite{Hough} find the line segments present.  The detector returns lists of line segment endpoints, from which we calculate the rotation of each line segment modulo $ \frac{\pi}{2}$ .  For each Cubelet detected in template matching, we determined all the line segments within some radius of the Cubelet center, then averaged the calculated angles for those line segments to obtain the rotation angle for the cube. \\

As this rotation estimation method relies solely on the ability to detect the outlines of the cube, anything that hinders that would  affect the accuracy of this estimation method. Since our environment does not contain clutter, this was not an issue.

\subsection{Color Determination}
Once we had a list of prospective Cubelet locations, we found the average color around the center of each match, and compared that to the color averages of each template image, selecting the smallest difference as the matching color.  This step is highly dependant upon how well the center of the the Cubelet has been determined; a large enough offset will result in the background or a nearby Cubelet's pixels to be picked up as part of the average color evaluation. We hypothesize that this problem could be alleviated by using a set of color filters to filter out the table and the magnetic tabs.

\subsection{Depth Information}
Depth data is collected using the XBox Kinect sensor.  Since the depth information matrix is in the same shape the RGB color image information, we need only the $(x,y)$ coordinate of a pixel in the image to determine its corresponding depth value in the 16-bit coupled depth image. Due to noise in the image and time constraints, we did not complete depth estimation.

\section{Experiment}

To evaluate the quality of our match results, we manually labeled each test image with the following information for each Cubelet: position, color and rotation. Rotation estimation was done rotating the image in an image editing tool until a particular cube's edge was parallel to the x-axis, then marking down the rotation angle required.\\

\section{Results}
The algorithm was run on three different data sets, each generated at different portions of the development process.  The first data set was taken with a a 640x480 webcam, with a limited set of Cubelet types on a wood panel background.  The second data set was again taken with the webcam, but with a white-papered background.  This test set also used blue-spray-painted Cubelets, which were not used in other tests. The final data set was taken with the Kinect sensor's camera, over a gray table background with a fairly bright light source and with the a set of 9 different colored Cubelets.

\begin{figure*}[!t]
\centerline{
\subfloat[From Test Set 1]
{\includegraphics[width=2.5in]{ds1p8.jpg}}
\label{fig_first_case}
\hfil
\subfloat[From Test Set 3]{\includegraphics[width=2.5in]{ds3p9.jpg}%
\label{fig_second_case}}}
\caption{Successful Detections and Estimations}
\label{fig_sim}
\end{figure*}

\begin{figure*}[!t]
\centerline{
\subfloat[From Test Set 2]
{\includegraphics[width=2.5in]{ds2p7.jpg}}
\label{fig_first_case}
\hfil
\subfloat[From Test Set 3]{\includegraphics[width=2.5in]{ds3p8.jpg}%
\label{fig_second_case}}}
\caption{Unsuccessful Detections and Estimations}
\label{fig_sim}
\end{figure*}

\section{Discussion}

In terms of actually detecting the presence of Cubelets, our process does do a fairly good job.  The rare cases of a Cubelets being entirely missed in the initial detection stage tended to result from partially occluded cube profiles, or due to a cube's features being washed out by very bright light or rendered indistinct by poor lighting.  One particular set of cases to be noted is that of the missed light-green cubes in data set three.  Upon closer observation, it seems that when the image is converted to grayscale, the metal connectors and the colored parts of the Cubelet tend to map towards similar grayscale color values.  This made the internal features of the Cubelet face less distinct, and caused the Cubelet face detection to fail.\\

The color determination was not quite as accurate as we would have hoped.  This mainly occured with the clear and black cubes, which in dim lighting have somewhat similar colors.  A way to address this would be to instead compare color histograms of the match with ones obtained from the Cubelet templates.  This would differentiate the black and the clear cubes well, as the black cubes would have a spike in the lower end of the histogram, and the clear cubes would have a more spread out distribution.\\

The results of the second data set were worse than the results for the other data sets due to several factors.  The first issue comes from the blue spraypainted Cubelets. This color was not as consistent and their features were more subdued. This caused them to be occasionally misidentified or not identified at all.  The clutter in the scene also posed difficulty for our process.  The wires present matched the templates just enough to register as Cubelets, which unfortunately demonstrates that this process is not very robust in less controlled situations.\\

On some occasions, the rotation estimation failed to give any estimation at all.  This was due to a lack of strong edges detected within the proximity of a Cubelet's center point.  Thus, anything that could cause an edge to be less distinct could result in a failure of rotation detection.  Cubelets that were completely surrounded were completely prone to this, but both surrounded Cubelets and less distinct edge issues could possibly be solved by comparing to Cubelets that were close enough that they were likely attached to the undetermined Cubelet.

\subsection{Further Work}
The major limitation of our approach is that it requires a fixed overhead view of the scene.  This constraint allowed us to use template matching to find Cubelets easily within the scene, limits its usefulness on a robot that can look around in its environment.  Our methods do not work very well if the Cubelets were to be seen from non-normal angle.  This change in perspective would require us to have a more thorough method of pose estimation.  \\

In order to address these concerns, we would need to find an object recognition method that is robust against scale, rotation, brightness differences, and affine transformations.  As we have already found that SURF is insufficient, we would investigate matching methods using other feature detectors, such as MSER (Maximally-Stable Extremal Region) \cite{MSER}, which is very robust against affine transformations.\\

Given a set of matching points between a training image and the target image, we could find the transform relating those sets of points, and calculate the pose of those points in the camera space, provided that the camera parameters were known.  Many different methods for doing this exist, including POSIT \cite{POSIT}, which works on four non-coplanar points.

\begin{table}[!t]
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
\caption{Cube Center Localization}
\label{table_1}
\centering
\begin{tabular}{|c|c|c|}
\hline
 & Pixel Distance from Actual &  Standard Deviation\\
\hline
Set 1 (14 images) & 3.9 & 5.6\\
\hline
Set 2 (10 images) & 4.3 & 3.6\\
\hline
Set 3 (10 images) & 3.1 & 2.7\\
\hline
\end{tabular}
\end{table}

\begin{table}[!t]
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
\caption{Color Determination Accuracy}
\label{table_2}
\centering
\begin{tabular}{|c|c|}
\hline
 & Accuracy\\
\hline
Set 1 (14 images) & 95\% \\
\hline
Set 2 (10 images) & 47\% \\
\hline
Set 3 (10 images) & 84\% \\
\hline
\end{tabular}
\end{table}

\begin{table}[!t]
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
\caption{Cube Rotation Estimation}
\label{table_3}
\centering
\begin{tabular}{|c|c|c|}
\hline
 & Pixel Distance from Actual &  Standard Deviation\\
\hline
Set 1 (14 images) & 5.0$^{\circ}$ & 6.8$^{\circ}$\\
\hline
Set 2 (10 images) & 10.3$^{\circ}$ & 12.9$^{\circ}$\\
\hline
Set 3 (10 images) & 3.3$^{\circ}$ & 3.8$^{\circ}$\\
\hline
\end{tabular}
\end{table}

\begin{table}[!t]
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
\caption{Failures}
\label{table_4}
\centering
\begin{tabular}{|c|c|c|}
\hline
 & False Positives & Missed Cubes\\
\hline
Set 1 (14 images) & 0 & 0 \\
\hline
Set 2 (10 images) & 21 & 5\\
\hline
Set 3 (10 images) & 1 & 6 \\
\hline
\end{tabular}
\end{table}

\section{Conclusion}
We believe that the work done has demonstrated that template matching can be an effective method of object detection in the constrained circumstances tested.  In designing and testing this method, we have also identified concerns regarding object detection and recognition that can be addressed in further research that would extend the capabilities of this system past its operating constraints.

% conference papers do not normally have an appendix


% use section* for acknowledgement
\section*{Acknowledgment}

The authors would like to thank Professor Nikolaus Correll and the CSCI 7000 class.



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{1}

%\bibitem{IEEEhowto:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

\bibitem{Loy_Eklundh}
G. Loy and J. Eklundh, \emph{Detecting Symmetry and Symmetric Constellations of Features}
\relax ECCV 2006, Part II, LNCS 3952, pp. 508-–521, 2006.


\bibitem{Cubelets}
Eric Schweikardt. 2010. Modular robotics studio. In Proceedings of the fifth international conference on Tangible, embedded, and embodied interaction (TEI '11). ACM, New York, NY, USA, 353-356. DOI=10.1145/1935701.1935784 http://doi.acm.org/10.1145/1935701.1935784

\bibitem{Clam}
http://correll.cs.colorado.edu/clam/

\bibitem{ROS}
Morgan Quigley, Brian Gerkey, Ken Conley, Josh Faust, Tully Foote, Jeremy Leibs, Eric Berger, Rob Wheeler, Andrew Ng. "ROS: an open-source Robot Operating System". http://www.ros.org/

\bibitem{SIFT}
Lowe, David G. (1999). "Object recognition from local scale-invariant features". Proceedings of the International Conference on Computer 
Vision. 2. pp. 1150–1157. doi:10.1109/ICCV.1999.790410.

\bibitem{SURF}
Herbert Bay, Andreas Ess, Tinne Tuytelaars, Luc Van Gool "SURF: Speeded Up Robust Features", Computer Vision and Image Understanding (CVIU), Vol. 110, No. 3, pp. 346--359, 2008

\bibitem{Kinect}
http://www.xbox.com/kinect

\bibitem{OpenCV}
http://opencv.willowgarage.com/

\bibitem{OpenNI}
http://75.98.78.94/default.aspx

\bibitem{Canny}
Canny, J., A Computational Approach To Edge Detection, IEEE Trans. Pattern Analysis and Machine Intelligence, 8(6):679–698, 1986.

\bibitem{Sobel}
K. Engel (2006). Real-time volume graphics,. pp. 112–114.

\bibitem{Hough}
http://en.wikipedia.org/wiki/Hough\_transform

\cite{MSER}
Donoser, M. and Bischof, H. Efficient Maximally Stable Extremal Region (MSER) Tracking CVPR, 2006.

\bibitem{POSIT}
Daniel DeMenthon and Larry S. Davis, "Model-Based Object Pose in 25 Lines of Code", International Journal of Computer Vision, 15, pp. 123-141, June 1995.
\end{thebibliography}




% that's all folks
\end{document}
